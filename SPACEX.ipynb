{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1be673-db5a-48fa-ab02-96215f38b5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd050d5-cb73-478d-b47f-40994a364b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f2970c-b655-42cc-bd9c-6980f7079260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Starting Web Scraping ---\n",
      "Wikipedia page title: List of Falcon 9 and Falcon Heavy launches - Wikipedia\n",
      "Target Wikipedia table not found.\n",
      "--- Finished Web Scraping ---\n",
      "\n",
      "--- 2. Starting API Data Collection ---\n",
      "API request status: 200\n",
      "API Data Head (before wrangling):\n",
      "   FlightNumber        Date BoosterVersion  PayloadMass Orbit  \\\n",
      "0             1  2006-03-24       Falcon 1         20.0   LEO   \n",
      "1             2  2007-03-21       Falcon 1          NaN   LEO   \n",
      "2             4  2008-09-28       Falcon 1        165.0   LEO   \n",
      "3             5  2009-07-13       Falcon 1        200.0   LEO   \n",
      "4             6  2010-06-04       Falcon 9          NaN   LEO   \n",
      "\n",
      "        LaunchSite    Outcome  Flights  GridFins  Reused   Legs LandingPad  \\\n",
      "0  Kwajalein Atoll  None None        1     False   False  False       None   \n",
      "1  Kwajalein Atoll  None None        1     False   False  False       None   \n",
      "2  Kwajalein Atoll  None None        1     False   False  False       None   \n",
      "3  Kwajalein Atoll  None None        1     False   False  False       None   \n",
      "4     CCSFS SLC 40  None None        1     False   False  False       None   \n",
      "\n",
      "   Block  ReusedCount    Serial   Longitude   Latitude  \n",
      "0    NaN            0  Merlin1A  167.743129   9.047721  \n",
      "1    NaN            0  Merlin2A  167.743129   9.047721  \n",
      "2    NaN            0  Merlin2C  167.743129   9.047721  \n",
      "3    NaN            0  Merlin3C  167.743129   9.047721  \n",
      "4    1.0            0     B0003  -80.577366  28.561857  \n",
      "--- Finished API Data Collection ---\n",
      "\n",
      "--- 3. Starting Data Wrangling ---\n",
      "Missing PayloadMass before imputation: 6\n",
      "Missing PayloadMass after imputation: 0\n",
      "\n",
      "Landing Outcomes Counts:\n",
      "True ASDS      41\n",
      "None None      25\n",
      "True RTLS      14\n",
      "False ASDS      6\n",
      "True Ocean      5\n",
      "False Ocean     2\n",
      "False RTLS      1\n",
      "Name: Outcome, dtype: int64\n",
      "\n",
      "DataFrame Head with Class label:\n",
      "     Outcome  Class\n",
      "0  None None      0\n",
      "1  None None      0\n",
      "2  None None      0\n",
      "3  None None      0\n",
      "4  None None      0\n",
      "\n",
      "Success Rate (Class mean): 0.6382978723404256\n",
      "--- Finished Data Wrangling ---\n",
      "\n",
      "--- 4. Starting EDA and Visualization ---\n",
      "Saved plot: eda_flight_payload.png\n",
      "Saved plot: eda_flight_launchsite.png\n",
      "Saved plot: eda_payload_launchsite.png\n",
      "Saved plot: eda_orbit_success_rate.png\n",
      "Saved plot: eda_flight_orbit.png\n",
      "Saved plot: eda_payload_orbit.png\n",
      "Saved plot: eda_yearly_success_trend.png\n",
      "--- Finished EDA and Visualization ---\n",
      "\n",
      "--- 5. Starting Feature Engineering ---\n",
      "Feature Engineering Head (One-Hot Encoded):\n",
      "   FlightNumber  PayloadMass  Flights  Block  ReusedCount  Orbit_ES-L1  \\\n",
      "0             1    20.000000        1    NaN            0            0   \n",
      "1             2  5919.165341        1    NaN            0            0   \n",
      "2             4   165.000000        1    NaN            0            0   \n",
      "3             5   200.000000        1    NaN            0            0   \n",
      "4             6  5919.165341        1    1.0            0            0   \n",
      "\n",
      "   Orbit_GEO  Orbit_GTO  Orbit_HEO  Orbit_ISS  ...  Serial_Merlin1A  \\\n",
      "0          0          0          0          0  ...                1   \n",
      "1          0          0          0          0  ...                0   \n",
      "2          0          0          0          0  ...                0   \n",
      "3          0          0          0          0  ...                0   \n",
      "4          0          0          0          0  ...                0   \n",
      "\n",
      "   Serial_Merlin2A  Serial_Merlin2C  Serial_Merlin3C  GridFins_False  \\\n",
      "0                0                0                0               1   \n",
      "1                1                0                0               1   \n",
      "2                0                1                0               1   \n",
      "3                0                0                1               1   \n",
      "4                0                0                0               1   \n",
      "\n",
      "   GridFins_True  Reused_False  Reused_True  Legs_False  Legs_True  \n",
      "0              0             1            0           1          0  \n",
      "1              0             1            0           1          0  \n",
      "2              0             1            0           1          0  \n",
      "3              0             1            0           1          0  \n",
      "4              0             1            0           1          0  \n",
      "\n",
      "[5 rows x 88 columns]\n",
      "--- Finished Feature Engineering ---\n",
      "\n",
      "--- 6. Starting Machine Learning ---\n",
      "Features shape after standardization: (94, 88)\n",
      "Train shapes: X=(75, 88), Y=(75,)\n",
      "Test shapes: X=(19, 88), Y=(19,)\n",
      "\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/ipykernel_launcher.py:606: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:665: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_folds = np.zeros(n_samples, dtype=np.int)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:437: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_split.py:113: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/fixes.py:357: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if _joblib.__version__ >= LooseVersion('0.12'):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6009/3224359384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Added random_state for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0mlogreg_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m     \u001b[0mlogreg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LogReg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0mbest_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LogReg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \"\"\"\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    255\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Consolidated SpaceX Falcon 9 Landing Prediction Code.\n",
    "\n",
    "This script combines steps from multiple notebooks:\n",
    "1. Web Scraping from Wikipedia\n",
    "2. API Data Collection from SpaceX API\n",
    "3. Data Wrangling and Target Variable Creation\n",
    "4. Basic EDA and Visualization (Matplotlib/Seaborn)\n",
    "5. Feature Engineering (One-Hot Encoding)\n",
    "6. Machine Learning Pipeline (Standardization, Train/Test Split, Model Training/Tuning, Evaluation)\n",
    "7. Folium Map Visualization\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Corrected import for older scikit-learn versions\n",
    "from sklearn.metrics import confusion_matrix, jaccard_similarity_score, f1_score\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, MousePosition\n",
    "from folium.features import DivIcon\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import io # Required for loading data from URL in some environments\n",
    "\n",
    "# Suppress potential warnings (optional)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "# From Web Scraping Notebook\n",
    "def date_time(table_cells):\n",
    "    \"\"\"\n",
    "    This function returns the data and time from the HTML table cell\n",
    "    Input: the element of a table data cell extracts extra row\n",
    "    \"\"\"\n",
    "    return [data_time.strip() for data_time in list(table_cells.strings)][0:2]\n",
    "\n",
    "def booster_version(table_cells):\n",
    "    \"\"\"\n",
    "    This function returns the booster version from the HTML table cell\n",
    "    Input: the element of a table data cell extracts extra row\n",
    "    \"\"\"\n",
    "    out = ''.join([booster_version for i, booster_version in enumerate(table_cells.strings) if i % 2 == 0][0:-1])\n",
    "    return out\n",
    "\n",
    "def landing_status(table_cells):\n",
    "    \"\"\"\n",
    "    This function returns the landing status from the HTML table cell\n",
    "    Input: the element of a table data cell extracts extra row\n",
    "    \"\"\"\n",
    "    out = [i for i in table_cells.strings][0]\n",
    "    return out\n",
    "\n",
    "def get_mass(table_cells):\n",
    "    mass = unicodedata.normalize(\"NFKD\", table_cells.text).strip()\n",
    "    if mass:\n",
    "        mass_find = mass.find(\"kg\") # Use a different variable name\n",
    "        if mass_find != -1:\n",
    "             new_mass = mass[0:mass_find + 2]\n",
    "        else:\n",
    "             new_mass = 0 # Handle cases where 'kg' is not found\n",
    "    else:\n",
    "        new_mass = 0\n",
    "    # Attempt to convert to float, handle potential errors\n",
    "    try:\n",
    "        # Extract numeric part before 'kg'\n",
    "        numeric_part = new_mass.replace('kg', '').replace(',', '').strip()\n",
    "        return float(numeric_part) if numeric_part else 0.0\n",
    "    except:\n",
    "        return 0.0 # Return 0 if conversion fails\n",
    "\n",
    "\n",
    "def extract_column_from_header(row):\n",
    "    \"\"\"\n",
    "    This function returns the column name from the HTML table header cell\n",
    "    Input: the element of a table header cell extracts extra row\n",
    "    \"\"\"\n",
    "    if row.br:\n",
    "        row.br.extract()\n",
    "    if row.a:\n",
    "        row.a.extract()\n",
    "    if row.sup:\n",
    "        row.sup.extract()\n",
    "\n",
    "    colunm_name = ' '.join(row.contents)\n",
    "\n",
    "    # Filter the digit and empty names\n",
    "    if not(colunm_name.strip().isdigit()):\n",
    "        colunm_name = colunm_name.strip()\n",
    "        return colunm_name\n",
    "\n",
    "# From API Data Collection Notebook\n",
    "def getBoosterVersion(data, BoosterVersionList):\n",
    "    for x in data['rocket']:\n",
    "        if x:\n",
    "            try:\n",
    "                response = requests.get(\"https://api.spacexdata.com/v4/rockets/\" + str(x))\n",
    "                response.raise_for_status() # Raise an exception for bad status codes\n",
    "                response_json = response.json()\n",
    "                BoosterVersionList.append(response_json.get('name', None)) # Use .get for safety\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"API request failed for rocket {x}: {e}\")\n",
    "                BoosterVersionList.append(None)\n",
    "            except ValueError: # Includes JSONDecodeError\n",
    "                print(f\"Failed to decode JSON for rocket {x}\")\n",
    "                BoosterVersionList.append(None)\n",
    "        else:\n",
    "            BoosterVersionList.append(None)\n",
    "\n",
    "def getLaunchSite(data, LongitudeList, LatitudeList, LaunchSiteList):\n",
    "    for x in data['launchpad']:\n",
    "        if x:\n",
    "            try:\n",
    "                response = requests.get(\"https://api.spacexdata.com/v4/launchpads/\" + str(x))\n",
    "                response.raise_for_status()\n",
    "                response_json = response.json()\n",
    "                LongitudeList.append(response_json.get('longitude', None))\n",
    "                LatitudeList.append(response_json.get('latitude', None))\n",
    "                LaunchSiteList.append(response_json.get('name', None))\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"API request failed for launchpad {x}: {e}\")\n",
    "                LongitudeList.append(None)\n",
    "                LatitudeList.append(None)\n",
    "                LaunchSiteList.append(None)\n",
    "            except ValueError:\n",
    "                print(f\"Failed to decode JSON for launchpad {x}\")\n",
    "                LongitudeList.append(None)\n",
    "                LatitudeList.append(None)\n",
    "                LaunchSiteList.append(None)\n",
    "        else:\n",
    "            LongitudeList.append(None)\n",
    "            LatitudeList.append(None)\n",
    "            LaunchSiteList.append(None)\n",
    "\n",
    "def getPayloadData(data, PayloadMassList, OrbitList):\n",
    "    for load in data['payloads']:\n",
    "        if load:\n",
    "            try:\n",
    "                response = requests.get(\"https://api.spacexdata.com/v4/payloads/\" + load)\n",
    "                response.raise_for_status()\n",
    "                response_json = response.json()\n",
    "                PayloadMassList.append(response_json.get('mass_kg', None))\n",
    "                OrbitList.append(response_json.get('orbit', None))\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"API request failed for payload {load}: {e}\")\n",
    "                PayloadMassList.append(None)\n",
    "                OrbitList.append(None)\n",
    "            except ValueError:\n",
    "                print(f\"Failed to decode JSON for payload {load}\")\n",
    "                PayloadMassList.append(None)\n",
    "                OrbitList.append(None)\n",
    "        else:\n",
    "             PayloadMassList.append(None)\n",
    "             OrbitList.append(None)\n",
    "\n",
    "def getCoreData(data, OutcomeList, FlightsList, GridFinsList, ReusedList, LegsList, LandingPadList, BlockList, ReusedCountList, SerialList):\n",
    "    for core in data['cores']:\n",
    "        if core.get('core') is not None: # Use .get for safety\n",
    "            try:\n",
    "                response = requests.get(\"https://api.spacexdata.com/v4/cores/\" + core['core'])\n",
    "                response.raise_for_status()\n",
    "                response_json = response.json()\n",
    "                BlockList.append(response_json.get('block', None))\n",
    "                ReusedCountList.append(response_json.get('reuse_count', None))\n",
    "                SerialList.append(response_json.get('serial', None))\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"API request failed for core {core.get('core')}: {e}\")\n",
    "                BlockList.append(None)\n",
    "                ReusedCountList.append(None)\n",
    "                SerialList.append(None)\n",
    "            except ValueError:\n",
    "                 print(f\"Failed to decode JSON for core {core.get('core')}\")\n",
    "                 BlockList.append(None)\n",
    "                 ReusedCountList.append(None)\n",
    "                 SerialList.append(None)\n",
    "        else:\n",
    "            BlockList.append(None)\n",
    "            ReusedCountList.append(None)\n",
    "            SerialList.append(None)\n",
    "\n",
    "        # Handle landing outcome string creation more robustly\n",
    "        landing_success = core.get('landing_success')\n",
    "        landing_type = core.get('landing_type')\n",
    "        outcome_str = f\"{landing_success} {landing_type}\" if landing_success is not None and landing_type is not None else \"None None\"\n",
    "        OutcomeList.append(outcome_str)\n",
    "\n",
    "        FlightsList.append(core.get('flight', None))\n",
    "        GridFinsList.append(core.get('gridfins', None))\n",
    "        ReusedList.append(core.get('reused', None))\n",
    "        LegsList.append(core.get('legs', None))\n",
    "        LandingPadList.append(core.get('landpad', None))\n",
    "\n",
    "# From Machine Learning Notebook\n",
    "def plot_confusion_matrix(y, y_predict, title='Confusion Matrix'):\n",
    "    \"\"\"this function plots the confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y, y_predict)\n",
    "    plt.figure(figsize=(6, 5)) # Added figure creation\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax=ax, fmt='g', cmap='Blues') # Added fmt and cmap\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_ticklabels(['did not land', 'landed'])\n",
    "    ax.yaxis.set_ticklabels(['did not land', 'landed'])\n",
    "    plt.tight_layout() # Added tight layout\n",
    "    # plt.show() # Display plot immediately (optional, depends on environment)\n",
    "\n",
    "# From Folium Notebook\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# --- 1. Web Scraping ---\n",
    "print(\"--- 1. Starting Web Scraping ---\")\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "try:\n",
    "    wiki_response = requests.get(static_url)\n",
    "    wiki_response.raise_for_status()\n",
    "    soup = BeautifulSoup(wiki_response.text, 'html.parser')\n",
    "    print(f\"Wikipedia page title: {soup.title.string if soup.title else 'No Title Found'}\")\n",
    "\n",
    "    html_tables = soup.find_all('table')\n",
    "    # The target table is usually the 3rd one with this specific class\n",
    "    first_launch_table = None\n",
    "    for table in html_tables:\n",
    "        # Check if the table has the specific classes\n",
    "        table_classes = table.get('class', [])\n",
    "        if 'wikitable' in table_classes and 'plainrowheaders' in table_classes and 'collapsible' in table_classes:\n",
    "             # Heuristic: Check for expected header content more robustly\n",
    "             th_texts = [th.get_text(strip=True) for th in table.find_all('th', scope='col')]\n",
    "             # Check for a few key headers that are likely to be present\n",
    "             if 'Flight No.' in th_texts and 'Launch site' in th_texts and 'Payload' in th_texts and 'Boosterlanding' in ''.join(th_texts).replace('<br/>',''):\n",
    "                 first_launch_table = table\n",
    "                 break # Found the likely table\n",
    "\n",
    "    if first_launch_table:\n",
    "        print(\"Found the target launch table.\")\n",
    "        column_names = []\n",
    "        # Find the first 'tr' which usually contains the headers\n",
    "        header_row = first_launch_table.find('tr')\n",
    "        if header_row:\n",
    "            for element in header_row.find_all('th'):\n",
    "                name = extract_column_from_header(element)\n",
    "                if name is not None and len(name) > 0:\n",
    "                    column_names.append(name)\n",
    "        print(f\"Extracted column names: {column_names}\")\n",
    "\n",
    "        # Initialize dictionary for scraped data\n",
    "        # Use a standard set of expected columns based on later processing steps\n",
    "        expected_columns = [\n",
    "            'Flight No.', 'Launch site', 'Payload', 'Payload mass', 'Orbit',\n",
    "            'Customer', 'Launch outcome', 'Version Booster', 'Booster landing',\n",
    "            'Date', 'Time'\n",
    "        ]\n",
    "        scraped_launch_dict = {name: [] for name in expected_columns}\n",
    "\n",
    "\n",
    "        extracted_row = 0\n",
    "        # Iterate through table rows, skipping the header row if necessary\n",
    "        data_rows = first_launch_table.find_all(\"tr\")\n",
    "        if data_rows and data_rows[0].find_all('th'): # Check if first row is header\n",
    "            data_rows = data_rows[1:] # Skip header row\n",
    "\n",
    "        for rows in data_rows:\n",
    "            # Check if the first element is a 'th' (like flight number) or 'td'\n",
    "            first_cell = rows.find(['th', 'td'])\n",
    "            if not first_cell: continue # Skip empty rows\n",
    "\n",
    "            flight_number_text = first_cell.get_text(strip=True)\n",
    "            flag = flight_number_text.isdigit()\n",
    "\n",
    "            # Get all 'td' elements in the row\n",
    "            row_cells = rows.find_all('td')\n",
    "\n",
    "            # If it's a numbered row and has enough cells\n",
    "            if flag and len(row_cells) >= 9:\n",
    "                extracted_row += 1\n",
    "                try:\n",
    "                    # Flight Number value\n",
    "                    scraped_launch_dict['Flight No.'].append(flight_number_text)\n",
    "\n",
    "                    # Date and Time values\n",
    "                    datatimelist = date_time(row_cells[0])\n",
    "                    date = datatimelist[0].strip(',') if len(datatimelist) > 0 else None\n",
    "                    time = datatimelist[1] if len(datatimelist) > 1 else None\n",
    "                    scraped_launch_dict['Date'].append(date)\n",
    "                    scraped_launch_dict['Time'].append(time)\n",
    "\n",
    "                    # Booster version\n",
    "                    bv = booster_version(row_cells[1])\n",
    "                    if not bv and row_cells[1].find('a'): # Check if 'a' tag exists\n",
    "                        bv = row_cells[1].find('a').string\n",
    "                    scraped_launch_dict['Version Booster'].append(bv)\n",
    "\n",
    "                    # Launch Site\n",
    "                    launch_site = row_cells[2].find('a').string if row_cells[2].find('a') else None\n",
    "                    scraped_launch_dict['Launch site'].append(launch_site)\n",
    "\n",
    "                    # Payload\n",
    "                    payload = row_cells[3].find('a').string if row_cells[3].find('a') else None\n",
    "                    scraped_launch_dict['Payload'].append(payload)\n",
    "\n",
    "                    # Payload Mass\n",
    "                    payload_mass = get_mass(row_cells[4])\n",
    "                    scraped_launch_dict['Payload mass'].append(payload_mass)\n",
    "\n",
    "                    # Orbit\n",
    "                    orbit = row_cells[5].find('a').string if row_cells[5].find('a') else None\n",
    "                    scraped_launch_dict['Orbit'].append(orbit)\n",
    "\n",
    "                    # Customer - Handle potential missing 'a' tag\n",
    "                    customer_cell = row_cells[6]\n",
    "                    customer_link = customer_cell.find('a')\n",
    "                    customer = customer_link.string if customer_link else customer_cell.get_text(strip=True) # Fallback to cell text\n",
    "                    scraped_launch_dict['Customer'].append(customer if customer else None)\n",
    "\n",
    "\n",
    "                    # Launch outcome\n",
    "                    launch_outcome = list(row_cells[7].strings)[0].strip() if list(row_cells[7].strings) else None\n",
    "                    scraped_launch_dict['Launch outcome'].append(launch_outcome)\n",
    "\n",
    "                    # Booster landing\n",
    "                    booster_landing = landing_status(row_cells[8])\n",
    "                    scraped_launch_dict['Booster landing'].append(booster_landing)\n",
    "\n",
    "                except IndexError as ie:\n",
    "                     print(f\"Index error parsing row {extracted_row} (Flight No. {flight_number_text}): {ie}. Row cells: {len(row_cells)}\")\n",
    "                     # Append None to maintain structure if error occurs\n",
    "                     for key in scraped_launch_dict:\n",
    "                         if len(scraped_launch_dict[key]) < extracted_row:\n",
    "                             scraped_launch_dict[key].append(None)\n",
    "                except Exception as e:\n",
    "                    print(f\"General error parsing row {extracted_row} (Flight No. {flight_number_text}): {e}\")\n",
    "                    # Append None to maintain structure if error occurs\n",
    "                    for key in scraped_launch_dict:\n",
    "                        if len(scraped_launch_dict[key]) < extracted_row:\n",
    "                            scraped_launch_dict[key].append(None)\n",
    "\n",
    "        # Check if data was extracted\n",
    "        if extracted_row > 0:\n",
    "             # Ensure all lists have the same length before creating DataFrame\n",
    "             max_len = max(len(lst) for lst in scraped_launch_dict.values())\n",
    "             for key in scraped_launch_dict:\n",
    "                 if len(scraped_launch_dict[key]) < max_len:\n",
    "                     scraped_launch_dict[key].extend([None] * (max_len - len(scraped_launch_dict[key])))\n",
    "\n",
    "             df_scraped = pd.DataFrame(scraped_launch_dict)\n",
    "             print(\"Wikipedia Scraped Data Head:\")\n",
    "             print(df_scraped.head())\n",
    "             # df_scraped.to_csv('spacex_web_scraped.csv', index=False) # Optional save\n",
    "        else:\n",
    "             print(\"No data extracted from Wikipedia table.\")\n",
    "             df_scraped = pd.DataFrame() # Create empty df if no data\n",
    "\n",
    "    else:\n",
    "        print(\"Target Wikipedia table not found.\")\n",
    "        df_scraped = pd.DataFrame() # Create empty df if no table\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Failed to fetch Wikipedia page: {e}\")\n",
    "    df_scraped = pd.DataFrame() # Create empty df on failure\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Wikipedia scraping: {e}\")\n",
    "    df_scraped = pd.DataFrame() # Create empty df on failure\n",
    "\n",
    "print(\"--- Finished Web Scraping ---\")\n",
    "\n",
    "\n",
    "# --- 2. API Data Collection ---\n",
    "print(\"\\n--- 2. Starting API Data Collection ---\")\n",
    "spacex_api_url = \"https://api.spacexdata.com/v4/launches/past\"\n",
    "try:\n",
    "    api_response = requests.get(spacex_api_url)\n",
    "    api_response.raise_for_status()\n",
    "    print(f\"API request status: {api_response.status_code}\")\n",
    "    api_data_raw = api_response.json()\n",
    "    api_data = pd.json_normalize(api_data_raw)\n",
    "\n",
    "    # Basic cleaning as per notebook\n",
    "    api_data = api_data[['rocket', 'payloads', 'launchpad', 'cores', 'flight_number', 'date_utc']]\n",
    "    # Handle potential errors if 'cores' or 'payloads' are not lists or are empty\n",
    "    api_data = api_data[api_data['cores'].apply(lambda x: isinstance(x, list) and len(x) == 1)]\n",
    "    api_data = api_data[api_data['payloads'].apply(lambda x: isinstance(x, list) and len(x) == 1)]\n",
    "    api_data['cores'] = api_data['cores'].map(lambda x: x[0] if x else None) # Extract first item safely\n",
    "    api_data['payloads'] = api_data['payloads'].map(lambda x: x[0] if x else None) # Extract first item safely\n",
    "    api_data['date'] = pd.to_datetime(api_data['date_utc']).dt.date\n",
    "    # Filter date as in notebook - Make sure 'date' column exists and is correct type\n",
    "    api_data = api_data[api_data['date'] <= datetime.date(2020, 11, 13)]\n",
    "\n",
    "    # Initialize lists for API data\n",
    "    BoosterVersion = []\n",
    "    PayloadMass = []\n",
    "    Orbit = []\n",
    "    LaunchSite = []\n",
    "    Outcome = []\n",
    "    Flights = []\n",
    "    GridFins = []\n",
    "    Reused = []\n",
    "    Legs = []\n",
    "    LandingPad = []\n",
    "    Block = []\n",
    "    ReusedCount = []\n",
    "    Serial = []\n",
    "    Longitude = []\n",
    "    Latitude = []\n",
    "\n",
    "    # Call helper functions to populate lists\n",
    "    getBoosterVersion(api_data, BoosterVersion)\n",
    "    getLaunchSite(api_data, Longitude, Latitude, LaunchSite)\n",
    "    getPayloadData(api_data, PayloadMass, Orbit)\n",
    "    getCoreData(api_data, Outcome, Flights, GridFins, Reused, Legs, LandingPad, Block, ReusedCount, Serial)\n",
    "\n",
    "    # Construct DataFrame from API lists\n",
    "    api_launch_dict = {'FlightNumber': list(api_data['flight_number']),\n",
    "                       'Date': list(api_data['date']),\n",
    "                       'BoosterVersion': BoosterVersion,\n",
    "                       'PayloadMass': PayloadMass,\n",
    "                       'Orbit': Orbit,\n",
    "                       'LaunchSite': LaunchSite,\n",
    "                       'Outcome': Outcome,\n",
    "                       'Flights': Flights,\n",
    "                       'GridFins': GridFins,\n",
    "                       'Reused': Reused,\n",
    "                       'Legs': Legs,\n",
    "                       'LandingPad': LandingPad,\n",
    "                       'Block': Block,\n",
    "                       'ReusedCount': ReusedCount,\n",
    "                       'Serial': Serial,\n",
    "                       'Longitude': Longitude,\n",
    "                       'Latitude': Latitude}\n",
    "\n",
    "    data_falcon9 = pd.DataFrame(api_launch_dict)\n",
    "    print(\"API Data Head (before wrangling):\")\n",
    "    print(data_falcon9.head())\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Failed to fetch data from SpaceX API: {e}\")\n",
    "    data_falcon9 = pd.DataFrame() # Create empty df on failure\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during API data processing: {e}\")\n",
    "    data_falcon9 = pd.DataFrame() # Create empty df on failure\n",
    "\n",
    "print(\"--- Finished API Data Collection ---\")\n",
    "\n",
    "\n",
    "# --- 3. Data Wrangling (using API data) ---\n",
    "print(\"\\n--- 3. Starting Data Wrangling ---\")\n",
    "if not data_falcon9.empty:\n",
    "    # Task 3: Dealing with Missing Values\n",
    "    print(f\"Missing PayloadMass before imputation: {data_falcon9['PayloadMass'].isnull().sum()}\")\n",
    "    payload_mass_mean = data_falcon9['PayloadMass'].mean()\n",
    "    data_falcon9['PayloadMass'].fillna(payload_mass_mean, inplace=True) # Use fillna instead of replace\n",
    "    print(f\"Missing PayloadMass after imputation: {data_falcon9['PayloadMass'].isnull().sum()}\")\n",
    "\n",
    "    # Task 4: Create Landing Outcome Label\n",
    "    landing_outcomes = data_falcon9['Outcome'].value_counts()\n",
    "    print(\"\\nLanding Outcomes Counts:\")\n",
    "    print(landing_outcomes)\n",
    "    # Define bad outcomes based on notebook logic\n",
    "    bad_outcomes = {'None None', 'False Ocean', 'False ASDS', 'False RTLS', 'None ASDS'}\n",
    "    landing_class = [0 if outcome in bad_outcomes else 1 for outcome in data_falcon9['Outcome']]\n",
    "    data_falcon9['Class'] = landing_class\n",
    "    print(\"\\nDataFrame Head with Class label:\")\n",
    "    print(data_falcon9[['Outcome', 'Class']].head())\n",
    "    print(f\"\\nSuccess Rate (Class mean): {data_falcon9['Class'].mean()}\")\n",
    "    # data_falcon9.to_csv('dataset_part_2.csv', index=False) # Optional save\n",
    "else:\n",
    "    print(\"Skipping Data Wrangling as API data loading failed.\")\n",
    "\n",
    "print(\"--- Finished Data Wrangling ---\")\n",
    "\n",
    "\n",
    "# --- 4. EDA and Visualization (using API data) ---\n",
    "print(\"\\n--- 4. Starting EDA and Visualization ---\")\n",
    "if not data_falcon9.empty:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.catplot(y=\"PayloadMass\", x=\"FlightNumber\", hue=\"Class\", data=data_falcon9, aspect=3, kind='strip') # Use strip plot\n",
    "    plt.xlabel(\"Flight Number\", fontsize=15)\n",
    "    plt.ylabel(\"Payload Mass (kg)\", fontsize=15)\n",
    "    plt.title(\"Flight Number vs. Payload Mass (Colored by Landing Success)\", fontsize=18)\n",
    "    plt.savefig(\"eda_flight_payload.png\") # Save plot\n",
    "    plt.close() # Close plot to prevent display issues in some environments\n",
    "    print(\"Saved plot: eda_flight_payload.png\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.catplot(x='FlightNumber', y='LaunchSite', hue='Class', data=data_falcon9, aspect=3, kind='strip')\n",
    "    plt.xlabel('Flight Number', fontsize=15)\n",
    "    plt.ylabel('Launch Site', fontsize=15)\n",
    "    plt.title(\"Flight Number vs. Launch Site (Colored by Landing Success)\", fontsize=18)\n",
    "    plt.savefig(\"eda_flight_launchsite.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: eda_flight_launchsite.png\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.catplot(x='PayloadMass', y='LaunchSite', hue='Class', data=data_falcon9, aspect=3, kind='strip')\n",
    "    plt.xlabel('Payload Mass (kg)', fontsize=15)\n",
    "    plt.ylabel('Launch Site', fontsize=15)\n",
    "    plt.title(\"Payload Mass vs. Launch Site (Colored by Landing Success)\", fontsize=18)\n",
    "    plt.savefig(\"eda_payload_launchsite.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: eda_payload_launchsite.png\")\n",
    "\n",
    "    # Success rate by Orbit\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    orbit_success_rate = data_falcon9.groupby('Orbit')['Class'].mean().reset_index()\n",
    "    sns.barplot(x='Orbit', y='Class', data=orbit_success_rate)\n",
    "    plt.xlabel('Orbit Type', fontsize=15)\n",
    "    plt.ylabel('Success Rate', fontsize=15)\n",
    "    plt.title('Launch Success Rate by Orbit Type', fontsize=18)\n",
    "    plt.xticks(rotation=45, ha='right') # Rotate labels if needed\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"eda_orbit_success_rate.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: eda_orbit_success_rate.png\")\n",
    "\n",
    "    # Flight Number vs Orbit\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.catplot(x='FlightNumber', y='Orbit', hue='Class', data=data_falcon9, aspect=3, kind='strip')\n",
    "    plt.xlabel('Flight Number', fontsize=15)\n",
    "    plt.ylabel('Orbit', fontsize=15)\n",
    "    plt.title('Flight Number vs. Orbit (Colored by Landing Success)', fontsize=18)\n",
    "    plt.savefig(\"eda_flight_orbit.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: eda_flight_orbit.png\")\n",
    "\n",
    "    # Payload vs Orbit\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.catplot(x='PayloadMass', y='Orbit', hue='Class', data=data_falcon9, aspect=3, kind='strip')\n",
    "    plt.xlabel('Payload Mass (kg)', fontsize=15)\n",
    "    plt.ylabel('Orbit', fontsize=15)\n",
    "    plt.title('Payload Mass vs. Orbit (Colored by Landing Success)', fontsize=18)\n",
    "    plt.savefig(\"eda_payload_orbit.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: eda_payload_orbit.png\")\n",
    "\n",
    "    # Yearly success rate trend\n",
    "    # Extract year (ensure Date column is datetime or string YYYY-MM-DD)\n",
    "    try:\n",
    "        # Attempt conversion if not already datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(data_falcon9['Date']):\n",
    "             data_falcon9['Date'] = pd.to_datetime(data_falcon9['Date'])\n",
    "        data_falcon9['Year'] = data_falcon9['Date'].dt.year\n",
    "        yearly_success = data_falcon9.groupby('Year')['Class'].mean().reset_index()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(x='Year', y='Class', data=yearly_success, marker='o') # Added marker\n",
    "        plt.xlabel('Year', fontsize=15)\n",
    "        plt.ylabel('Average Success Rate', fontsize=15)\n",
    "        plt.title('Launch Success Rate Trend Over Years', fontsize=18)\n",
    "        plt.grid(True)\n",
    "        plt.ylim(0, 1.05) # Set y-axis limits\n",
    "        plt.xticks(yearly_success['Year'].unique()) # Ensure all years are shown\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"eda_yearly_success_trend.png\")\n",
    "        plt.close()\n",
    "        print(\"Saved plot: eda_yearly_success_trend.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot yearly trend: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping EDA and Visualization as data loading failed.\")\n",
    "\n",
    "print(\"--- Finished EDA and Visualization ---\")\n",
    "\n",
    "\n",
    "# --- 5. Feature Engineering (One-Hot Encoding) ---\n",
    "print(\"\\n--- 5. Starting Feature Engineering ---\")\n",
    "if not data_falcon9.empty:\n",
    "    # Select features for modeling, excluding Date, Outcome, BoosterVersion, Year\n",
    "    features = data_falcon9[['FlightNumber', 'PayloadMass', 'Orbit', 'LaunchSite', 'Flights', 'GridFins', 'Reused', 'Legs', 'LandingPad', 'Block', 'ReusedCount', 'Serial']]\n",
    "    categorical_cols = ['Orbit', 'LaunchSite', 'LandingPad', 'Serial', 'GridFins', 'Reused', 'Legs'] # Include boolean cols for get_dummies\n",
    "\n",
    "    # Convert boolean columns to string before get_dummies if they exist\n",
    "    for col in ['GridFins', 'Reused', 'Legs']:\n",
    "        if col in features.columns:\n",
    "            features[col] = features[col].astype(str)\n",
    "\n",
    "    features_one_hot = pd.get_dummies(features, columns=categorical_cols)\n",
    "    print(\"Feature Engineering Head (One-Hot Encoded):\")\n",
    "    print(features_one_hot.head())\n",
    "    # features_one_hot.to_csv('dataset_part_3.csv', index=False) # Optional save\n",
    "else:\n",
    "    print(\"Skipping Feature Engineering as data loading failed.\")\n",
    "    features_one_hot = pd.DataFrame() # Define as empty for ML section check\n",
    "\n",
    "print(\"--- Finished Feature Engineering ---\")\n",
    "\n",
    "\n",
    "# --- 6. Machine Learning Prediction ---\n",
    "print(\"\\n--- 6. Starting Machine Learning ---\")\n",
    "if not features_one_hot.empty and 'Class' in data_falcon9.columns:\n",
    "    # Task 1: Create Y array\n",
    "    Y = data_falcon9['Class'].to_numpy()\n",
    "\n",
    "    # Task 2: Standardize X\n",
    "    X = features_one_hot.astype(float) # Ensure float type\n",
    "    transform = preprocessing.StandardScaler()\n",
    "    X = transform.fit_transform(X)\n",
    "    print(f\"Features shape after standardization: {X.shape}\")\n",
    "\n",
    "    # Task 3: Train/Test Split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
    "    print(f\"Train shapes: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "    print(f\"Test shapes: X={X_test.shape}, Y={Y_test.shape}\")\n",
    "\n",
    "    # --- Model Training and Evaluation ---\n",
    "    models = {}\n",
    "    best_scores = {}\n",
    "    test_accuracies = {}\n",
    "    predictions = {}\n",
    "\n",
    "    # Task 4 & 5: Logistic Regression\n",
    "    print(\"\\nTraining Logistic Regression...\")\n",
    "    parameters_lr = {'C': [0.01, 0.1, 1], 'penalty': ['l2'], 'solver': ['lbfgs']}\n",
    "    lr = LogisticRegression(random_state=1) # Added random_state for reproducibility\n",
    "    logreg_cv = GridSearchCV(lr, parameters_lr, cv=10)\n",
    "    logreg_cv.fit(X_train, Y_train)\n",
    "    models['LogReg'] = logreg_cv\n",
    "    best_scores['LogReg'] = logreg_cv.best_score_\n",
    "    test_accuracies['LogReg'] = logreg_cv.score(X_test, Y_test)\n",
    "    predictions['LogReg'] = logreg_cv.predict(X_test)\n",
    "    print(f\"LogReg Best Params: {logreg_cv.best_params_}\")\n",
    "    print(f\"LogReg CV Accuracy: {best_scores['LogReg']:.4f}\")\n",
    "    print(f\"LogReg Test Accuracy: {test_accuracies['LogReg']:.4f}\")\n",
    "    plot_confusion_matrix(Y_test, predictions['LogReg'], title='LogReg Confusion Matrix')\n",
    "    plt.savefig(\"cm_logreg.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: cm_logreg.png\")\n",
    "\n",
    "\n",
    "    # Task 6 & 7: Support Vector Machine (SVM)\n",
    "    print(\"\\nTraining SVM...\")\n",
    "    parameters_svm = {'kernel': ('linear', 'rbf', 'poly', 'sigmoid'), # Removed duplicate 'rbf'\n",
    "                      'C': np.logspace(-3, 3, 5),\n",
    "                      'gamma': np.logspace(-3, 3, 5)}\n",
    "    svm = SVC(random_state=1) # Added random_state\n",
    "    svm_cv = GridSearchCV(svm, parameters_svm, cv=10)\n",
    "    svm_cv.fit(X_train, Y_train)\n",
    "    models['SVM'] = svm_cv\n",
    "    best_scores['SVM'] = svm_cv.best_score_\n",
    "    test_accuracies['SVM'] = svm_cv.score(X_test, Y_test)\n",
    "    predictions['SVM'] = svm_cv.predict(X_test)\n",
    "    print(f\"SVM Best Params: {svm_cv.best_params_}\")\n",
    "    print(f\"SVM CV Accuracy: {best_scores['SVM']:.4f}\")\n",
    "    print(f\"SVM Test Accuracy: {test_accuracies['SVM']:.4f}\")\n",
    "    plot_confusion_matrix(Y_test, predictions['SVM'], title='SVM Confusion Matrix')\n",
    "    plt.savefig(\"cm_svm.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: cm_svm.png\")\n",
    "\n",
    "    # Task 8 & 9: Decision Tree\n",
    "    print(\"\\nTraining Decision Tree...\")\n",
    "    parameters_tree = {'criterion': ['gini', 'entropy'],\n",
    "                       'splitter': ['best', 'random'],\n",
    "                       'max_depth': [2 * n for n in range(1, 10)],\n",
    "                       'max_features': ['auto', 'sqrt'], # 'auto' is deprecated, often equivalent to 'sqrt' or None\n",
    "                       'min_samples_leaf': [1, 2, 4],\n",
    "                       'min_samples_split': [2, 5, 10]}\n",
    "    tree = DecisionTreeClassifier(random_state=1) # Added random_state\n",
    "    tree_cv = GridSearchCV(tree, parameters_tree, cv=10)\n",
    "    tree_cv.fit(X_train, Y_train)\n",
    "    models['Tree'] = tree_cv\n",
    "    best_scores['Tree'] = tree_cv.best_score_\n",
    "    test_accuracies['Tree'] = tree_cv.score(X_test, Y_test)\n",
    "    predictions['Tree'] = tree_cv.predict(X_test)\n",
    "    print(f\"Tree Best Params: {tree_cv.best_params_}\")\n",
    "    print(f\"Tree CV Accuracy: {best_scores['Tree']:.4f}\")\n",
    "    print(f\"Tree Test Accuracy: {test_accuracies['Tree']:.4f}\")\n",
    "    plot_confusion_matrix(Y_test, predictions['Tree'], title='Decision Tree Confusion Matrix')\n",
    "    plt.savefig(\"cm_tree.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: cm_tree.png\")\n",
    "\n",
    "    # Task 10 & 11: K-Nearest Neighbors (KNN)\n",
    "    print(\"\\nTraining KNN...\")\n",
    "    parameters_knn = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                      'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                      'p': [1, 2]}\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_cv = GridSearchCV(knn, parameters_knn, cv=10)\n",
    "    knn_cv.fit(X_train, Y_train)\n",
    "    models['KNN'] = knn_cv\n",
    "    best_scores['KNN'] = knn_cv.best_score_\n",
    "    test_accuracies['KNN'] = knn_cv.score(X_test, Y_test)\n",
    "    predictions['KNN'] = knn_cv.predict(X_test)\n",
    "    print(f\"KNN Best Params: {knn_cv.best_params_}\")\n",
    "    print(f\"KNN CV Accuracy: {best_scores['KNN']:.4f}\")\n",
    "    print(f\"KNN Test Accuracy: {test_accuracies['KNN']:.4f}\")\n",
    "    plot_confusion_matrix(Y_test, predictions['KNN'], title='KNN Confusion Matrix')\n",
    "    plt.savefig(\"cm_knn.png\")\n",
    "    plt.close()\n",
    "    print(\"Saved plot: cm_knn.png\")\n",
    "\n",
    "    # Task 12: Find Best Method\n",
    "    print(\"\\n--- Model Comparison (Test Set) ---\")\n",
    "    # Use jaccard_similarity_score for older sklearn versions\n",
    "    jaccard_scores_test = [jaccard_similarity_score(Y_test, yhat) for yhat in predictions.values()]\n",
    "    f1_scores_test = [f1_score(Y_test, yhat, average='binary') for yhat in predictions.values()]\n",
    "    accuracy_test = list(test_accuracies.values())\n",
    "    model_names = list(models.keys())\n",
    "\n",
    "    scores_test_df = pd.DataFrame(\n",
    "        np.array([jaccard_scores_test, f1_scores_test, accuracy_test]),\n",
    "        index=['Jaccard_Score', 'F1_Score', 'Accuracy'],\n",
    "        columns=model_names\n",
    "    )\n",
    "    print(\"\\nTest Set Scores:\")\n",
    "    print(scores_test_df)\n",
    "\n",
    "    # Compare based on F1-score (often good for imbalanced classes) or Accuracy\n",
    "    best_f1_model = model_names[np.argmax(f1_scores_test)]\n",
    "    best_acc_model = model_names[np.argmax(accuracy_test)]\n",
    "    print(f\"\\nBest model based on Test F1-Score: {best_f1_model} ({max(f1_scores_test):.4f})\")\n",
    "    print(f\"Best model based on Test Accuracy: {best_acc_model} ({max(accuracy_test):.4f})\")\n",
    "\n",
    "    # Also show scores on the whole dataset as done in the notebook\n",
    "    print(\"\\n--- Model Comparison (Whole Dataset) ---\")\n",
    "    # Use jaccard_similarity_score for older sklearn versions\n",
    "    jaccard_scores_all = [jaccard_similarity_score(Y, cv.predict(X)) for cv in models.values()]\n",
    "    f1_scores_all = [f1_score(Y, cv.predict(X), average='binary') for cv in models.values()]\n",
    "    accuracy_all = [cv.score(X, Y) for cv in models.values()]\n",
    "\n",
    "    scores_all_df = pd.DataFrame(\n",
    "        np.array([jaccard_scores_all, f1_scores_all, accuracy_all]),\n",
    "        index=['Jaccard_Score', 'F1_Score', 'Accuracy'],\n",
    "        columns=model_names\n",
    "    )\n",
    "    print(\"\\nWhole Dataset Scores:\")\n",
    "    print(scores_all_df)\n",
    "    best_f1_model_all = model_names[np.argmax(f1_scores_all)]\n",
    "    best_acc_model_all = model_names[np.argmax(accuracy_all)]\n",
    "    print(f\"\\nBest model based on Whole Dataset F1-Score: {best_f1_model_all} ({max(f1_scores_all):.4f})\")\n",
    "    print(f\"Best model based on Whole Dataset Accuracy: {best_acc_model_all} ({max(accuracy_all):.4f})\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Machine Learning as data preparation failed.\")\n",
    "\n",
    "print(\"--- Finished Machine Learning ---\")\n",
    "\n",
    "\n",
    "# --- 7. Folium Map Visualization ---\n",
    "print(\"\\n--- 7. Starting Folium Map Visualization ---\")\n",
    "# Use the data_falcon9 DataFrame created earlier which includes Lat, Long, LaunchSite, Class\n",
    "if not data_falcon9.empty and all(col in data_falcon9.columns for col in ['Lat', 'Long', 'LaunchSite', 'Class']):\n",
    "    # Get unique launch sites coordinates\n",
    "    # Drop rows with NaN Lat/Long before grouping\n",
    "    launch_sites_df_valid = data_falcon9.dropna(subset=['Lat', 'Long'])\n",
    "    launch_sites_df = launch_sites_df_valid.groupby(['LaunchSite'], as_index=False).first()[['LaunchSite', 'Lat', 'Long']]\n",
    "\n",
    "    # Initial map centered roughly around US launch sites\n",
    "    if not launch_sites_df.empty:\n",
    "        center_lat = launch_sites_df['Lat'].mean()\n",
    "        center_long = launch_sites_df['Long'].mean()\n",
    "    else:\n",
    "        center_lat = 29.55 # Default if no sites found\n",
    "        center_long = -95.08\n",
    "\n",
    "    site_map = folium.Map(location=[center_lat, center_long], zoom_start=4)\n",
    "\n",
    "    # Add circles and labels for each launch site\n",
    "    for launch_site, site_lat, site_long in zip(launch_sites_df['LaunchSite'], launch_sites_df['Lat'], launch_sites_df['Long']):\n",
    "        site_coordinate = [site_lat, site_long]\n",
    "        circle = folium.Circle(site_coordinate, radius=1000, color='#d35400', fill=True).add_child(folium.Popup(launch_site))\n",
    "        marker = folium.map.Marker(\n",
    "            site_coordinate,\n",
    "            icon=DivIcon(\n",
    "                icon_size=(20, 20),\n",
    "                icon_anchor=(0, 0),\n",
    "                html='<div style=\"font-size: 12; color:#d35400;\"><b>%s</b></div>' % launch_site,\n",
    "            )\n",
    "        )\n",
    "        site_map.add_child(circle)\n",
    "        site_map.add_child(marker)\n",
    "\n",
    "    # Add success/fail markers with clustering\n",
    "    marker_cluster = MarkerCluster().add_to(site_map)\n",
    "    # Ensure marker_color exists, handle potential NaNs in Lat/Long for iteration\n",
    "    if 'marker_color' not in data_falcon9.columns:\n",
    "         data_falcon9['marker_color'] = data_falcon9['Class'].apply(lambda x: 'green' if x == 1 else 'red')\n",
    "\n",
    "    for index, record in data_falcon9.dropna(subset=['Lat', 'Long']).iterrows(): # Iterate over valid rows\n",
    "        site_coordinate = [record['Lat'], record['Long']]\n",
    "        marker = folium.Marker(\n",
    "            location=site_coordinate,\n",
    "            icon=folium.Icon(color='white', icon_color=record['marker_color']),\n",
    "            popup=record['LaunchSite'] # Add popup for site name\n",
    "        )\n",
    "        marker_cluster.add_child(marker)\n",
    "\n",
    "    # Add Mouse Position control\n",
    "    formatter = \"function(num) {return L.Util.formatNum(num, 5);};\"\n",
    "    mouse_position = MousePosition(\n",
    "        position='topright',\n",
    "        separator=' Long: ',\n",
    "        empty_string='NaN',\n",
    "        lng_first=False,\n",
    "        num_digits=20,\n",
    "        prefix='Lat:',\n",
    "        lat_formatter=formatter,\n",
    "        lng_formatter=formatter,\n",
    "    )\n",
    "    site_map.add_child(mouse_position)\n",
    "\n",
    "    # Add distance lines (example for KSC LC-39A)\n",
    "    try:\n",
    "        ksc_row = launch_sites_df[launch_sites_df['LaunchSite'] == 'KSC LC-39A']\n",
    "        if not ksc_row.empty:\n",
    "            ksc_coords = ksc_row[['Lat', 'Long']].iloc[0].tolist()\n",
    "            # Define some points of interest near KSC (approximate coordinates)\n",
    "            coastline_ksc = [28.573, -80.568] # Approx East coastline\n",
    "            railway_ksc = [28.573, -80.800] # Approx railway line west of KSC\n",
    "            highway_ksc = [28.573, -80.853] # Approx US-1\n",
    "            city_ksc = [28.612, -80.808] # Titusville\n",
    "\n",
    "            points_of_interest = {\n",
    "                \"Coastline\": coastline_ksc,\n",
    "                \"Railway\": railway_ksc,\n",
    "                \"Highway\": highway_ksc,\n",
    "                \"City (Titusville)\": city_ksc\n",
    "            }\n",
    "            colors = ['blue', 'green', 'purple', 'orange']\n",
    "            html_colors = ['#007bff','#28a745','#6f42c1','#fd7e14']\n",
    "\n",
    "            for i, (name, coords) in enumerate(points_of_interest.items()):\n",
    "                distance = calculate_distance(ksc_coords[0], ksc_coords[1], coords[0], coords[1])\n",
    "                # Distance Marker\n",
    "                folium.Marker(\n",
    "                    coords,\n",
    "                    icon=DivIcon(\n",
    "                        icon_size=(150,30), # Adjusted size\n",
    "                        icon_anchor=(0,0),\n",
    "                        html=f'<div style=\"font-size: 12px; color:{html_colors[i]};\"><b>{name} ({distance:.2f} km)</b></div>',\n",
    "                    )\n",
    "                ).add_to(site_map)\n",
    "                # Distance Line\n",
    "                folium.PolyLine([ksc_coords, coords], color=colors[i]).add_to(site_map)\n",
    "        else:\n",
    "            print(\"Could not find KSC LC-39A coordinates to draw distance lines.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding distance lines: {e}\")\n",
    "\n",
    "    # Save map to HTML\n",
    "    site_map.save(\"launch_sites_analysis_map.html\")\n",
    "    print(\"Saved Folium map to launch_sites_analysis_map.html\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Folium Map Visualization as data loading/wrangling failed.\")\n",
    "\n",
    "print(\"--- Finished Folium Map Visualization ---\")\n",
    "\n",
    "print(\"\\n--- Project Script Finished ---\")\n",
    "# Note: The Dash application code (spacex_app.py) needs to be run separately as a web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678569d-0ea7-4993-ac18-8d2d6791fada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
